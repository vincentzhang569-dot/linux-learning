import streamlit as st
from core.llm_client import get_client
from core.rag_bridge import build_vector_store, query_vector_store
import pdfplumber  # è®°å¾—ç¡®ä¿å®‰è£…äº†è¿™ä¸ªåº“ï¼špip install pdfplumber

# --- 1. æ ¸å¿ƒå˜é‡åˆå§‹åŒ– ---
if "messages" not in st.session_state:
    st.session_state.messages = []

if "knowledge_base_ready" not in st.session_state:
    st.session_state.knowledge_base_ready = False  # æ ‡è®°çŸ¥è¯†åº“æ˜¯å¦å·²æ„å»º

# --- 2. ä¾§è¾¹æ ï¼šæ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ (RAG å‡çº§ç‰ˆ) ---
with st.sidebar:
    st.header("ğŸ“‚ çŸ¥è¯†åº“æŒ‚è½½")
    st.caption("ä¸Šä¼ æŠ€æœ¯æ‰‹å†Œ/ç»´ä¿®æ–‡æ¡£ï¼ŒAI å°†åŸºäºæ–‡æ¡£å›ç­”ã€‚")
    
    uploaded_file = st.file_uploader("ä¸Šä¼  PDF æ–‡æ¡£", type=["pdf"])
    
    # å¤„ç†æ–‡ä»¶ä¸Šä¼ é€»è¾‘
    if uploaded_file is not None:
        try:
            with pdfplumber.open(uploaded_file) as pdf:
                # æå–æ‰€æœ‰é¡µé¢çš„æ–‡æœ¬
                all_text = ""
                for page in pdf.pages:
                    all_text += page.extract_text() + "\n"
                
                # è°ƒç”¨ RAG æ„å»ºå‘é‡å­˜å‚¨
                with st.spinner("æ­£åœ¨æ„å»ºçŸ¥è¯†åº“ç´¢å¼•..."):
                    result = build_vector_store(all_text)
                    if result.startswith("âœ…"):
                        st.session_state.knowledge_base_ready = True
                        st.success(result)
                    else:
                        st.session_state.knowledge_base_ready = False
                        st.warning(result)
        except Exception as e:
            st.error(f"âŒ è§£æå¤±è´¥: {e}")
            st.session_state.knowledge_base_ready = False
    else:
        # å¦‚æœç”¨æˆ·ç§»é™¤æ–‡ä»¶ï¼Œé‡ç½®çŸ¥è¯†åº“çŠ¶æ€
        st.session_state.knowledge_base_ready = False

    st.divider()
    
    # å¼ºåˆ¶æ¸…ç©ºæŒ‰é’®
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯ / é‡ç½®", type="primary", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# --- 3. åŸºç¡€ System Prompt æ¨¡æ¿ ---
base_system_prompt = """
ä½ æ˜¯ä¸€ä½å·¥ä¸šç»´ä¿®ä¸“å®¶ã€‚
è¯·åŸºäºä»¥ä¸‹ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ä½¿ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†è¡¥å……ï¼Œä½†è¦è¯´æ˜"èµ„æ–™ä¸­æœªæåŠ"ã€‚
"""


# --- 4. ä¸»ç•Œé¢å¸ƒå±€ ---
st.title("ğŸ­ å·¥ä¸šäººå·¥æ™ºèƒ½å¤§è„‘")

# === å…³é”®ä¿®å¤ï¼šå…ˆæ˜¾ç¤ºå†å²è®°å½•ï¼Œå†å¤„ç†æ–°è¾“å…¥ ===
# (æŠŠè¿™æ®µä»£ç æŒªåˆ°ä¸Šé¢ï¼Œè§£å†³äº†â€œå›ç­”ä¸¤æ¬¡â€çš„ BUG)
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- 5. å¤„ç†èŠå¤©çš„å‡½æ•° ---
def handle_chat(user_input):
    # 1. æ—¢ç„¶ä¸Šé¢å·²ç»æ˜¾ç¤ºäº†å†å²ï¼Œè¿™é‡Œåªéœ€è¦æ˜¾ç¤º"æ–°çš„ä¸€è½®"
    # A. æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    with st.chat_message("user"):
        st.markdown(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})

    # B. æ˜¾ç¤º AI å›å¤
    with st.chat_message("assistant"):
        # 2. RAG æŸ¥è¯¢ï¼šä»å‘é‡åº“ä¸­æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
        context = ""
        if st.session_state.knowledge_base_ready:
            context = query_vector_store(user_input, k=3)
        
        # 3. æ„å»ºåŠ¨æ€ System Prompt
        if context:
            final_system_content = f"""
{base_system_prompt}

ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
{context}
"""
        else:
            final_system_content = base_system_prompt
        
        system_prompt = {"role": "system", "content": final_system_content}
        
        # 4. è°ƒç”¨ AI
        client = get_client()
        # æ„é€ æ¶ˆæ¯ï¼šç³»ç»Ÿè®¾å®š + å†å²è®°å½•
        api_messages = [system_prompt] + st.session_state.messages
        
        response = client.chat.completions.create(
            model="glm-4-flash",
            messages=api_messages,
            stream=True,
            temperature=0.1
        )
        full_response = st.write_stream(response)
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})


# --- 6. å¿«æ·æŒ‰é’®åŒº ---
st.markdown("##### âš¡ å¿«é€Ÿè¯Šæ–­é€šé“")
col1, col2, col3, col4 = st.columns(4)

def quick_action(prompt):
    # å¼ºåˆ¶æ¸…ç©ºå†å²ï¼Œé˜²æ­¢ä¸²å°
    st.session_state.messages = []
    # å¼ºåˆ¶åˆ·æ–°é¡µé¢ï¼Œè®©ä¸Šé¢çš„å†å²è®°å½•åŒºæ¸…ç©º
    # ä½†ä¸ºäº†èƒ½æ‰§è¡Œ handle_chatï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç‚¹å°æŠ€å·§ï¼š
    # ç›´æ¥åœ¨è¿™é‡Œè°ƒç”¨ handle_chatï¼Œå› ä¸º session_state å·²ç»æ¸…ç©ºï¼Œä¸Šé¢å¾ªç¯ä¸ä¼šæ‰“å°æ—§çš„
    handle_chat(prompt)
    # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦ rerunï¼Œå› ä¸º handle_chat ä¼šå®æ—¶ç”»å‡ºæ¥

if col1.button("ä¼ºæœç”µæœºæ•…éšœ", use_container_width=True):
    quick_action("æˆ‘çš„è®¾å¤‡å‡ºç°äº†ã€ä¼ºæœç”µæœºæ•…éšœã€‘ã€‚è¯·è¯¦ç»†åˆ—å‡ºï¼šç¡¬ä»¶æ£€æŸ¥ã€ç”µæ°”æ£€æŸ¥ã€å‚æ•°è®¾ç½®ä¸‰æ–¹é¢çš„æ’æŸ¥æ­¥éª¤ã€‚")

if col2.button("é€šè®¯è¶…æ—¶", use_container_width=True):
    quick_action("æˆ‘çš„è®¾å¤‡å‡ºç°äº†ã€PLCé€šè®¯è¶…æ—¶ã€‘ã€‚è¯·è¯¦ç»†åˆ—å‡ºï¼šç‰©ç†è¿æ¥ã€ç½‘ç»œé…ç½®ã€å¹²æ‰°æ’æŸ¥ä¸‰æ–¹é¢çš„æ’æŸ¥æ­¥éª¤ã€‚")

if col3.button("ABBæœºå™¨äººé”™è¯¯", use_container_width=True):
    quick_action("æˆ‘çš„ABBæœºå™¨äººæŠ¥é”™ã€‚è¯·åˆ—å‡ºæœ€å¸¸è§çš„5ä¸ªé”™è¯¯ä»£ç åŠå…¶å«ä¹‰å’Œè§£å†³åŠæ³•ã€‚")

if col4.button("ç¼–ç å™¨å¼‚å¸¸", use_container_width=True):
    quick_action("æˆ‘çš„è®¾å¤‡æŠ¥ã€ç¼–ç å™¨æ•…éšœã€‘ã€‚è¯·åˆ—å‡ºæ’æŸ¥æ­¥éª¤ï¼ˆçº¿è·¯ã€ç”µæ± ã€æœºæ¢°å®‰è£…ï¼‰ã€‚")

# --- 7. åº•éƒ¨è¾“å…¥æ¡† ---
if user_input := st.chat_input("è¯·è¾“å…¥å…·ä½“æ•…éšœç°è±¡ï¼Œæˆ–ä¸Šä¼ æ–‡æ¡£åæé—®..."):
    handle_chat(user_input)
