import streamlit as st
from core.llm_client import get_client
from core.rag_bridge import build_vector_store, query_vector_store
import pdfplumber  # è®°å¾—ç¡®ä¿å®‰è£…äº†è¿™ä¸ªåº“ï¼špip install pdfplumber

# --- 1. æ ¸å¿ƒå˜é‡åˆå§‹åŒ– ---
if "messages" not in st.session_state:
    st.session_state.messages = []

if "knowledge_base_ready" not in st.session_state:
    st.session_state.knowledge_base_ready = False  # æ ‡è®°çŸ¥è¯†åº“æ˜¯å¦å·²æ„å»º

# --- 2. ä¾§è¾¹æ ï¼šæ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ (RAG å‡çº§ç‰ˆ) ---
with st.sidebar:
    st.header("ğŸ“‚ çŸ¥è¯†åº“æŒ‚è½½")
    st.caption("ä¸Šä¼ æŠ€æœ¯æ‰‹å†Œ/ç»´ä¿®æ–‡æ¡£ï¼ŒAI å°†åŸºäºæ–‡æ¡£å›ç­”ã€‚")
    
    # === ä¾§è¾¹æ ï¼šæ–‡æ¡£ä¸Šä¼ åŒº ===
    uploaded_file = st.file_uploader("ä¸Šä¼  PDF æ–‡æ¡£", type=["pdf"])
    
    # å¤„ç†æ–‡ä»¶ä¸Šä¼ 
    if uploaded_file is not None:
        # å®šä¹‰æœ€å¤§é¡µæ•°é™åˆ¶ (ä¿æŠ¤ 2G å†…å­˜æœåŠ¡å™¨)
        MAX_PAGES = 50 
        
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡è¿™ä¸ªæ–‡ä»¶ï¼Œé˜²æ­¢é‡å¤è®¡ç®—
            if "last_uploaded" not in st.session_state or st.session_state.last_uploaded != uploaded_file.name:
                
                # 1. è¿›åº¦æ¡ç»„ä»¶
                progress_bar = st.progress(0, text="æ­£åœ¨å¯åŠ¨æ–‡æ¡£è§£æå¼•æ“...")
                text = ""
                
                with pdfplumber.open(uploaded_file) as pdf:
                    total_pages = len(pdf.pages)
                    # å¦‚æœé¡µæ•°å¤ªå¤šï¼Œå¼ºåˆ¶æˆªæ–­
                    process_pages = min(total_pages, MAX_PAGES)
                    
                    if total_pages > MAX_PAGES:
                        st.warning(f"âš ï¸ æ–‡æ¡£è¿‡å¤§ ({total_pages}é¡µ)ï¼Œä¸ºé˜²æ­¢æœåŠ¡å™¨å´©æºƒï¼Œä»…è¯»å–å‰ {MAX_PAGES} é¡µã€‚")
                    
                    # 2. é€é¡µè¯»å–å¹¶æ›´æ–°è¿›åº¦æ¡
                    for i in range(process_pages):
                        page_text = pdf.pages[i].extract_text()
                        if page_text:
                            text += page_text + "\n"
                        
                        # æ›´æ–°è¿›åº¦ (0% - 50%)
                        current_progress = int((i / process_pages) * 50)
                        progress_bar.progress(current_progress, text=f"æ­£åœ¨è¯»å–ç¬¬ {i+1}/{process_pages} é¡µ...")
                
                # 3. æ„å»ºå‘é‡åº“ (è€—æ—¶æ“ä½œ)
                if text:
                    progress_bar.progress(60, text="æ­£åœ¨åˆ‡åˆ†æ–‡æœ¬å¹¶æ„å»ºå‘é‡ç´¢å¼• (è¿™éœ€è¦ä¸€ç‚¹æ—¶é—´)...")
                    
                    # è°ƒç”¨ core é‡Œçš„å‡½æ•°
                    result_msg = build_vector_store(text)
                    
                    # å®Œæˆ
                    progress_bar.progress(100, text="âœ… å¤„ç†å®Œæˆï¼")
                    st.success(result_msg)
                    
                    # è®°å½•çŠ¶æ€
                    if result_msg.startswith("âœ…"):
                        st.session_state.knowledge_base_ready = True
                    else:
                        st.session_state.knowledge_base_ready = False
                    st.session_state.doc_content = text # (å¯é€‰ï¼šå­˜åŸæ–‡ä»¥ä¾¿æŸ¥çœ‹ï¼Œå¦‚æœå†…å­˜ç´§å¼ å¯æ³¨é‡Šæ‰è¿™è¡Œ)
                    st.session_state.last_uploaded = uploaded_file.name
                
        except Exception as e:
            st.error(f"æ–‡æ¡£è¯»å–å¤±è´¥: {e}")
            st.session_state.knowledge_base_ready = False
    else:
        # å¦‚æœç”¨æˆ·ç§»é™¤æ–‡ä»¶ï¼Œé‡ç½®çŸ¥è¯†åº“çŠ¶æ€
        st.session_state.knowledge_base_ready = False

    st.divider()
    
    # å¼ºåˆ¶æ¸…ç©ºæŒ‰é’®
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯ / é‡ç½®", type="primary", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# --- 3. åŸºç¡€ System Prompt æ¨¡æ¿ ---
base_system_prompt = """
ä½ æ˜¯ä¸€ä½å·¥ä¸šç»´ä¿®ä¸“å®¶ã€‚
è¯·åŸºäºä»¥ä¸‹ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ä½¿ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†è¡¥å……ï¼Œä½†è¦è¯´æ˜"èµ„æ–™ä¸­æœªæåŠ"ã€‚
"""


# --- 4. ä¸»ç•Œé¢å¸ƒå±€ ---
st.title("ğŸ­ å·¥ä¸šäººå·¥æ™ºèƒ½å¤§è„‘")

# === å…³é”®ä¿®å¤ï¼šå…ˆæ˜¾ç¤ºå†å²è®°å½•ï¼Œå†å¤„ç†æ–°è¾“å…¥ ===
# (æŠŠè¿™æ®µä»£ç æŒªåˆ°ä¸Šé¢ï¼Œè§£å†³äº†â€œå›ç­”ä¸¤æ¬¡â€çš„ BUG)
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- 5. å¤„ç†èŠå¤©çš„å‡½æ•° ---
def handle_chat(user_input):
    # 1. æ—¢ç„¶ä¸Šé¢å·²ç»æ˜¾ç¤ºäº†å†å²ï¼Œè¿™é‡Œåªéœ€è¦æ˜¾ç¤º"æ–°çš„ä¸€è½®"
    # A. æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    with st.chat_message("user"):
        st.markdown(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})

    # B. æ˜¾ç¤º AI å›å¤
    with st.chat_message("assistant"):
        # 2. RAG æŸ¥è¯¢ï¼šä»å‘é‡åº“ä¸­æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
        context = ""
        if st.session_state.knowledge_base_ready:
            context = query_vector_store(user_input, k=3)
        
        # 3. æ„å»ºåŠ¨æ€ System Prompt
        if context:
            final_system_content = f"""
{base_system_prompt}

ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
{context}
"""
        else:
            final_system_content = base_system_prompt
        
        system_prompt = {"role": "system", "content": final_system_content}
        
        # 4. è°ƒç”¨ AI
        client = get_client()
        # æ„é€ æ¶ˆæ¯ï¼šç³»ç»Ÿè®¾å®š + å†å²è®°å½•
        api_messages = [system_prompt] + st.session_state.messages
        
        response = client.chat.completions.create(
            model="glm-4-flash",
            messages=api_messages,
            stream=True,
            temperature=0.1
        )
        full_response = st.write_stream(response)
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})


# --- 6. å¿«æ·æŒ‰é’®åŒº ---
st.markdown("##### âš¡ å¿«é€Ÿè¯Šæ–­é€šé“")
col1, col2, col3, col4 = st.columns(4)

def quick_action(prompt):
    # å¼ºåˆ¶æ¸…ç©ºå†å²ï¼Œé˜²æ­¢ä¸²å°
    st.session_state.messages = []
    # å¼ºåˆ¶åˆ·æ–°é¡µé¢ï¼Œè®©ä¸Šé¢çš„å†å²è®°å½•åŒºæ¸…ç©º
    # ä½†ä¸ºäº†èƒ½æ‰§è¡Œ handle_chatï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç‚¹å°æŠ€å·§ï¼š
    # ç›´æ¥åœ¨è¿™é‡Œè°ƒç”¨ handle_chatï¼Œå› ä¸º session_state å·²ç»æ¸…ç©ºï¼Œä¸Šé¢å¾ªç¯ä¸ä¼šæ‰“å°æ—§çš„
    handle_chat(prompt)
    # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦ rerunï¼Œå› ä¸º handle_chat ä¼šå®æ—¶ç”»å‡ºæ¥

if col1.button("ä¼ºæœç”µæœºæ•…éšœ", use_container_width=True):
    quick_action("æˆ‘çš„è®¾å¤‡å‡ºç°äº†ã€ä¼ºæœç”µæœºæ•…éšœã€‘ã€‚è¯·è¯¦ç»†åˆ—å‡ºï¼šç¡¬ä»¶æ£€æŸ¥ã€ç”µæ°”æ£€æŸ¥ã€å‚æ•°è®¾ç½®ä¸‰æ–¹é¢çš„æ’æŸ¥æ­¥éª¤ã€‚")

if col2.button("é€šè®¯è¶…æ—¶", use_container_width=True):
    quick_action("æˆ‘çš„è®¾å¤‡å‡ºç°äº†ã€PLCé€šè®¯è¶…æ—¶ã€‘ã€‚è¯·è¯¦ç»†åˆ—å‡ºï¼šç‰©ç†è¿æ¥ã€ç½‘ç»œé…ç½®ã€å¹²æ‰°æ’æŸ¥ä¸‰æ–¹é¢çš„æ’æŸ¥æ­¥éª¤ã€‚")

if col3.button("ABBæœºå™¨äººé”™è¯¯", use_container_width=True):
    quick_action("æˆ‘çš„ABBæœºå™¨äººæŠ¥é”™ã€‚è¯·åˆ—å‡ºæœ€å¸¸è§çš„5ä¸ªé”™è¯¯ä»£ç åŠå…¶å«ä¹‰å’Œè§£å†³åŠæ³•ã€‚")

if col4.button("ç¼–ç å™¨å¼‚å¸¸", use_container_width=True):
    quick_action("æˆ‘çš„è®¾å¤‡æŠ¥ã€ç¼–ç å™¨æ•…éšœã€‘ã€‚è¯·åˆ—å‡ºæ’æŸ¥æ­¥éª¤ï¼ˆçº¿è·¯ã€ç”µæ± ã€æœºæ¢°å®‰è£…ï¼‰ã€‚")

# --- 7. åº•éƒ¨è¾“å…¥æ¡† ---
if user_input := st.chat_input("è¯·è¾“å…¥å…·ä½“æ•…éšœç°è±¡ï¼Œæˆ–ä¸Šä¼ æ–‡æ¡£åæé—®..."):
    handle_chat(user_input)
